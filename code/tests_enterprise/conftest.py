#/Users/w.worawan/Downloads/ai-operation-microservice3_v2ori/code/tests_enterprise/conftest.py
from __future__ import annotations

import pytest

from model.conversation_state import ConversationState
from model.persona_supervisor import PersonaSupervisor

from .fakes import SpyRetriever, LLMCallStats, FakeLLMJSON


@pytest.fixture()
def retriever():
    return SpyRetriever()


@pytest.fixture()
def llm_stats():
    return LLMCallStats()


@pytest.fixture()
def llm_router():
    """
    Router returns JSON for:
    - intent classifier: {intent, meta}
    - confirm yes/no: {yes,no,confidence}
    - style wants_long/short: {wants_long,wants_short,confidence}
    - academic final answer: decision JSON
    """

    def _router(prompt: str):
        p = (prompt or "").lower()

        # ------------------------------------------------------------
        # 1) YES/NO confirmation classifier (persona switch confirm)
        # ------------------------------------------------------------
        if ("yes" in p and "no" in p) and ("confidence" in p) and ("‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°" in prompt or "classify" in p):
            if any(x in p for x in ["‡πÄ‡∏¢‡∏õ", "yep", "‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢", "‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô", "ok", "okay", "‡∏à‡∏±‡∏î‡πÑ‡∏õ", "‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°", "‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö"]):
                return {"yes": True, "no": False, "confidence": 0.9}
            if any(x in p for x in ["‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤", "‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å", "cancel", "nope", "‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á", "‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏±‡∏ö"]):
                return {"yes": False, "no": True, "confidence": 0.9}
            return {"yes": False, "no": False, "confidence": 0.2}

        # ------------------------------------------------------------
        # 2) Style classifier (wants_long / wants_short)
        # ------------------------------------------------------------
        if ("wants_long" in p and "wants_short" in p) and ("confidence" in p):
            if any(x in p for x in ["‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", "‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å", "‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á", "‡∏ï‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", "‡∏•‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", "reasoning"]):
                return {"wants_long": True, "wants_short": False, "confidence": 0.9}
            if any(x in p for x in ["‡∏™‡∏±‡πâ‡∏ô", "‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö", "‡∏™‡∏£‡∏∏‡∏õ", "‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡πÜ", "tl;dr"]):
                return {"wants_long": False, "wants_short": True, "confidence": 0.9}
            return {"wants_long": False, "wants_short": False, "confidence": 0.2}

        # ------------------------------------------------------------
        # 3) INTENT classifier (CRITICAL for enterprise suite)
        # Expect: { "intent": "...", "meta": {...} }
        # We detect this prompt by presence of "intent" and "meta"
        # ------------------------------------------------------------
        if ("intent" in p and "meta" in p) and ("return json" in p or "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô json" in p or "json" in p):
            # normalize user_text extraction: take the last non-empty line as proxy
            user_text = ""
            for line in (prompt or "").splitlines()[::-1]:
                t = line.strip()
                if t:
                    user_text = t
                    break
            u = user_text.lower()

            # explicit switch
            if any(x in u for x in ["‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÇ‡∏´‡∏°‡∏î", "‡∏™‡∏•‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î", "switch mode", "switch persona"]):
                # no target
                if not any(x in u for x in ["practical", "academic", "‡∏™‡∏±‡πâ‡∏ô", "‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"]):
                    return {"intent": "explicit_switch", "meta": {"kind": "no_target"}}
                # has target implied by style
                if any(x in u for x in ["‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", "‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å", "academic"]):
                    return {"intent": "explicit_switch", "meta": {"kind": "target", "wants_long": True}}
                if any(x in u for x in ["‡∏™‡∏±‡πâ‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ", "practical"]):
                    return {"intent": "explicit_switch", "meta": {"kind": "target", "wants_short": True}}
                return {"intent": "explicit_switch", "meta": {"kind": "no_target"}}

            # mode status
            if any(x in u for x in ["‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÇ‡∏´‡∏°‡∏î", "‡∏≠‡∏¢‡∏π‡πà‡πÇ‡∏´‡∏°‡∏î", "mode status", "‡πÇ‡∏´‡∏°‡∏î‡∏≠‡∏∞‡πÑ‡∏£"]):
                return {"intent": "mode_status", "meta": {}}

            # greeting / thanks / noise
            if any(x in u for x in ["‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "hi", "hello", "‡∏î‡∏µ‡∏à‡πâ‡∏≤", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "thanks", "thank you"]):
                return {"intent": "greeting", "meta": {}}
            if any(x in u for x in ["555", "lol", "lmao", "haha", "‡∏Æ‡πà‡∏≤", "üòÖ", "üòÇ"]):
                return {"intent": "noise", "meta": {}}

            # legal (must retrieve)
            if any(x in u for x in ["‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô", "‡∏ô‡∏≤‡∏¢‡∏à‡πâ‡∏≤‡∏á", "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô", "‡∏†‡∏û.20", "vat", "‡∏†‡∏≤‡∏©‡∏µ", "‡πÉ‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï"]):
                return {"intent": "legal", "meta": {}}

            # default
            return {"intent": "unknown", "meta": {}}

        # ------------------------------------------------------------
        # 4) Academic final answer JSON
        # ------------------------------------------------------------
        if "return json" in p and "documents" in p and "user_question" in p:
            return {
                "input_type": "new_question",
                "analysis": "ok",
                "action": "answer",
                "execution": {
                    "answer": "‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö academic ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: ...\n- ‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°",
                    "context_update": {},
                },
            }

        # default: harmless
        return {"ok": True}

    return _router


@pytest.fixture()
def supervisor(monkeypatch, retriever, llm_stats, llm_router):
    """
    Supervisor + patch ChatOpenAI.invoke globally to deterministic FakeLLMJSON
    so test suite never calls real LLM.
    """
    from langchain_openai import ChatOpenAI

    fake = FakeLLMJSON(llm_stats, llm_router)
    monkeypatch.setattr(ChatOpenAI, "invoke", fake.invoke, raising=True)

    sup = PersonaSupervisor(retriever=retriever)
    return sup


@pytest.fixture()
def new_state():
    def _make(persona_id: str = "practical"):
        return ConversationState(
            session_id="test",
            persona_id=persona_id,
            context={},
            messages=[],
            internal_messages=[],
        )
    return _make